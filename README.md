
# Purpose:


A startup called **Sparkify** wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs, users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. 

The purpose of this project is to extract data from the json files and store it in the database tables so that the analytics team can query and analyze the data.

## Datasets:


The following two datasets are used:

**Song dataset**: 
Dataset contains metadata about a song and the artist of that song. Each file is in JSON format. The files are partitioned by the first three letters of each song's track ID.
***Location***: data\song_data
***Sample record from a file data\song_data\A\A\A\TRAAAAW128F428D538.json:***

{"num_songs": 1, "artist_id": "ARD7TVE1187B99BFB1", "artist_latitude": null, "artist_longitude": null, "artist_location": "California - LA", "artist_name": "Casual", "song_id":  "SOMZWCG12A8C13C480", "title": "I Didn't Mean To", "duration": 218.93179, "year": 0}

**Log Dataset**: 
Dataset consists of log files generated by the simulator based on the songs in the Song dataset. Each file is in JSON format. The files are partitioned by year and month.
***Location***: data\log_data
***Sample record from a file data\log_data\2018\11\2018-11-01-events.json:***

{"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null,"level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0,"sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"}


## Execution steps:


1. First, run create_tables (**command**: `python create_tables.py`) script in the terminal. This script will drop the tables (songplays, users, songs, artists, time) if already existing in the workspace and will create the same tables for inserting the records.
2. Next, run etl (**command**: `python etl.py`) script in the terminal. This script will process the JSON files and insert the data to all the corresponding tables.


## Design:


![ERD](/images/postgres.png)

In this project, four dimensional tables (songs, artists, users and time) and one fact table (songplays) were created.
Unique values from the song json file were inserted into the respective columns of the songs and the artist table. Similarly, unique values from the log json file were inserted into the respective columns of the users and the time table. These tables provide the detailed information of an entity and can be queried individually for analysis as well as for quicker response. 
eg: 
1. How many songs are in the database? 
2. How many artists are there? 
Songplays table connects the information from the dimension tables through the key columns. The table provides complete information about the songs, artists, users, play time etc.
Through etl pipeline, process would connect to the database, drop or create necessary tables, extract the json files and finally insert the records to the respective tables.
